{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1a30f0-801f-42b6-956f-796196a99c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.3 Joaquin Quintana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff606787-0ce5-49f9-88e9-376fe0b48a7e",
   "metadata": {},
   "source": [
    "![title](\"IMG-0068.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964217c6-da85-411a-b827-813eee1f3485",
   "metadata": {},
   "source": [
    "__a.__ True - K is always sqaure. Think about this from the point of view of $A$ and $A^T$ which makes up two sections of the block matrix and have shapes $m*n$ and $n*m$. Then to make up the last part of the block we must have the sizes which are missing which are m*m and $n*n$ in size. These missing sizes nicely match the size of the Identity matrix and the zero matrix. This can be seen in the picture below. Additionally, the identity matrix always has shape $n*n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818bb3a-6e93-4ff0-abee-11c3d56d294f",
   "metadata": {},
   "source": [
    "__b.__ True - from the image we see that A has all the same length sides which are of size n+m. This implies A is a square. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab70b6-305f-4d05-818a-0c6dd0cddf67",
   "metadata": {},
   "source": [
    "__c.__ True - when the matrix is transposed the Identity matrix and zero matrix end up in the same location. Additionally, when we transpose $A^T$ we get back $A$ which ends up in the place where matrix $A$ was before being transposed and vice versa. This is easy to demonstate in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c29643-b5fc-43e0-a2b7-21ad4b9d3779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...  3. 11.  5.]\n",
      " [ 0.  1.  0. ... 10. 14.  3.]\n",
      " [ 0.  0.  1. ... 10.  5.  0.]\n",
      " ...\n",
      " [ 3. 10. 10. ...  0.  0.  0.]\n",
      " [11. 14.  5. ...  0.  0.  0.]\n",
      " [ 5.  3.  0. ...  0.  0.  0.]] \n",
      "\n",
      "Is the transposed block and orginal block matrix equivalent :  True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#create a block vector which matches the problem parameters\n",
    "def blockMatrix(col,row):\n",
    "    I = np.identity(row) #identity matrix of size m * m\n",
    "    A = np.random.randint(15, size=(col, row)) #create a matrix A of size m * n. Place random ints between 0 and 15 \n",
    "    Z = np.zeros([col,col]) # zero matrix of size n * n\n",
    "    block = np.block([[I,A.T],[A,Z]]) #create our block matrix\n",
    "    return block\n",
    "\n",
    "Block = blockMatrix(15,120)\n",
    "Transposed_Block = Block.T\n",
    "#check that transposed matrix is equivalent. This is K == K^T\n",
    "print(Block,\"\\n\")\n",
    "print(\"Is the transposed block and orginal block matrix equivalent : \",np.allclose(Block,Transposed_Block))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799dfb8-106f-45c7-92c5-52b023e5c659",
   "metadata": {},
   "source": [
    "__d.__ False - They only have the same dimensions when $A$ is sqaure or when $m = n$ (number of rows equal the number of columns). All other times the identity matrix has size $n*n$ and the zero matrix has size $m*m$. The image below helps show this.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd8c74-42bd-45ab-a0b5-d0e556e42ddb",
   "metadata": {},
   "source": [
    "__e.__ True - This is covered in d but the zero matrix is in the right corner and neighboring $A$ and $A^T$ which makes both of its sides of size m. Therefore this zero matrix is square.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d55d1-c390-41db-9a15-d2ee099b770a",
   "metadata": {},
   "source": [
    "# 3. 6.9\n",
    "\n",
    "This problem looks at the 3 vectors which are related to the reach matrix R. \n",
    "\n",
    "a. Here we see n-vector c gives the total amount of money that the company put forth for all channels with each $c_i$ being the amount shelled out for the $i^{th}$ channel. To get the total amount invested for all n-channels we need to simply sum the vector. This is $totalInvestment = 1^Tc$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e37b48-baf7-463c-9dd3-9b80a86006de",
   "metadata": {},
   "source": [
    "b. We can see that these vectors are related and that the problem is setup to have the reach matrix return the ratio for the number of views in segment i for each dollar spent on channel j. This translates to $R_{i,j} = \\frac{v_i}{c_j}$ and gives us a nice ratio to determine how well this channel is doing in comparison to the money put in. So by way of simple algebraic manipulation we can say $v = Rc$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7b138-ce4d-4efc-ac93-e198b412cac4",
   "metadata": {},
   "source": [
    "c. Much like part b we are going to look at the vectors and how they relate to each other with regards to dimensional analysis. To find the total sales we will look at vectors v and a. Each $v_i$ is the number of impressions for segment i and $a_i$ is a ratio which gives the profit per impression in segment i.   \n",
    "\n",
    "By multiplying $v_i$ by $c_i$ we get the total profit for group i. For example...\n",
    "\n",
    "Let $ v_i = 2*imp$ __and__ $a_i = \\frac{50.00 usd}{1}*imp $\n",
    "\n",
    "__then__ $v_ia_i = \\frac{2*imp}{1}* \\frac {50.00 usd}{imp} = 100 usd$\n",
    "\n",
    "And to get the total profit for all groups we can simply do this elementwise for the two vectors. This is compute the inner product of v and a or $v^Ta$. Finally, from part b we see that by simple subtitution we get $Rc^Ta = a^TRc$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8f2bb-a9be-4bdc-a53d-5806d7ff8842",
   "metadata": {},
   "source": [
    "d. We are told $R_{i,j}$ gives the ratio of views in segment $i$ for dollars spent on channel $j$. So from this we need to look at $R_{3,j}$ or row 3 of the reach matrix $R$. We see that this ratio will be larger when views are high and dollars spent is low and low when views are low and dollars spent is high. Therefore this implies we are searching for the maximum element in Row 3 of the reach matrix $R$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad388b-ffb8-4540-870f-016db35f2f33",
   "metadata": {},
   "source": [
    "e. This is somewhat explained in part d and I'll elaborate from there. If $R_{3,5}$ is smaller than all other elements in $R$ we know there are low views in comparison to the amount of money put in incomparison to all other $R_{i,j} \\neq R_{3,5}$. This is, all other $R_{i,j}$ are larger which implies they all have more views than dollars put in in comparison to $R_{i,j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8825fc-0c72-4c71-bd31-5a4390ff68db",
   "metadata": {},
   "source": [
    "# 5 6.17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a9b79-faaa-4917-a8b2-9705bf954282",
   "metadata": {},
   "source": [
    "a. This problem is similar to 5.1 a but with matricies. This is, we know the identity matrix is linerally independent and can say that $\\alpha I = 0$ and therefore $\\alpha = 0$. Since these matricies are stacked we can represent them as follows ...  \n",
    "\n",
    "$S = \\alpha\\begin{bmatrix} A  \\\\ I \\end{bmatrix}= \\begin{bmatrix}  \\alpha A  \\\\  \\alpha I \\end{bmatrix} = 0$\n",
    "\n",
    "and since $Sx = 0$ we can say $S$ is linerally independent with coefficent $x = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e33c2-0267-492d-b2a0-a82fdda13cd8",
   "metadata": {},
   "source": [
    "b. Linear independence inequality says that if an n-vector x is composed of $a_1,...,a_k$ vecotrs and has $k \\leq n$ then x is a set of linearly independent vectors. If k = n+1 then we can say they are linearly dependent. We see there are m+n rows in $S$ and each row is of size n. Lets translate this a bit and let m+n equal k and we see that we have k>n or that that rows are linerally dependent. \n",
    "\n",
    "Below I confirm this result for S the Gram-Schmidt algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f39a22d-8c07-467a-89d9-4dbe90af4d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked matrix S = (A,I):  \n",
      " [[4. 5. 7. 5. 5.]\n",
      " [3. 7. 7. 0. 3.]\n",
      " [0. 0. 0. 3. 8.]\n",
      " [5. 0. 0. 4. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]] \n",
      "\n",
      "rows of S\n",
      "row 0:  [4. 5. 7. 5. 5.]\n",
      "row 1:  [3. 7. 7. 0. 3.]\n",
      "row 2:  [0. 0. 0. 3. 8.]\n",
      "row 3:  [5. 0. 0. 4. 0.]\n",
      "row 4:  [1. 0. 0. 0. 0.]\n",
      "row 5:  [0. 1. 0. 0. 0.]\n",
      "row 6:  [0. 0. 1. 0. 0.]\n",
      "row 7:  [0. 0. 0. 1. 0.]\n",
      "row 8:  [0. 0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "columns of S\n",
      "column 0:  [4. 3. 0. 5. 1. 0. 0. 0. 0.]\n",
      "column 1:  [5. 7. 0. 0. 0. 1. 0. 0. 0.]\n",
      "column 2:  [7. 7. 0. 0. 0. 0. 1. 0. 0.]\n",
      "column 3:  [5. 0. 3. 4. 0. 0. 0. 1. 0.]\n",
      "column 4:  [5. 3. 8. 0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "Running Gram-schmidt on rows of S\n",
      "Vectors are linearly dependent.\n",
      "GS algorithm terminates at iteration  6\n",
      "\n",
      "\n",
      "Running Gram-schmidt on columns of S\n",
      "Vectors are linearly independent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.56011203, 0.42008403, 0.        , 0.70014004, 0.14002801,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " array([ 0.27519714,  0.70764978,  0.        , -0.6199496 , -0.12398992,\n",
       "         0.15423136,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.58988218, -0.35057597,  0.        , -0.25150016, -0.05030003,\n",
       "        -0.49537909,  0.46685397,  0.        ,  0.        ]),\n",
       " array([ 0.26668575, -0.23280191,  0.78427138, -0.03061505, -0.21526204,\n",
       "         0.29618463, -0.23718688,  0.26142379,  0.        ]),\n",
       " array([-0.27567605,  0.2509569 ,  0.5933028 ,  0.00340607,  0.33280316,\n",
       "        -0.37831802,  0.17303408, -0.41515244,  0.22984501])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#function provided in python companion notebook\n",
    "def gram_schmidt(a):\n",
    "    q = []\n",
    "    for i in range(len(a)):\n",
    "    #orthogonalization\n",
    "        q_tilde = a[i]\n",
    "        for j in range(len(q)):\n",
    "            q_tilde = q_tilde - (q[j] @ a[i])*q[j]\n",
    "        #Test for dependennce\n",
    "        if np.sqrt(sum(q_tilde**2)) <= 1e-10:\n",
    "            print('Vectors are linearly dependent.')\n",
    "            print('GS algorithm terminates at iteration ', i+1)\n",
    "            return q\n",
    "            #Normalization\n",
    "        else:\n",
    "            q_tilde = q_tilde / np.sqrt(sum(q_tilde**2))\n",
    "            q.append(q_tilde)\n",
    "    print('Vectors are linearly independent.')\n",
    "    return q\n",
    "\n",
    "#define A, I and S which is the stacked vector\n",
    "I = np.identity(5) #identity matrix of size n * n\n",
    "A = np.random.randint(9, size=(4, 5)) #create a matrix A of size m * n. Place random ints between 0 and 9 \n",
    "S = np.concatenate([A,I],axis=0)\n",
    "ST = S.T\n",
    "n,m = S.shape\n",
    "\n",
    "print(\"Stacked matrix S = (A,I): \",\"\\n\",S,\"\\n\",)\n",
    "\n",
    "#print rows \n",
    "print(\"rows of S\")\n",
    "[print(\"row \" + str(row)+ \": \" , S[row]) for row in range(n)]\n",
    "print(\"\\n\")\n",
    "\n",
    "#print columns \n",
    "print(\"columns of S\")\n",
    "[print(\"column \"+ str(col)+ \": \" ,ST[col]) for col in range(m)]   \n",
    "print(\"\\n\")\n",
    "\n",
    "#Show the rows are lineraly dependent\n",
    "print(\"Running Gram-schmidt on rows of S\")\n",
    "gram_schmidt(S)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Show the columns are lineraly independent\n",
    "print(\"Running Gram-schmidt on columns of S\")\n",
    "gram_schmidt(S.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f948bc-7db4-4302-95f6-0335d6179287",
   "metadata": {},
   "source": [
    "# 6-6.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898bac6-8a15-4cb1-b486-78504f9c5406",
   "metadata": {},
   "source": [
    "__Info Needed__ \n",
    "* Additon and multiplication of two $m*n$ matricies each requires $mn$ flops\n",
    "* Adding vectors takes $n$ flops \n",
    "* Matrix-vector mulitplication takes $2nm$ flops\n",
    "\n",
    "We are given $z = (A+B)(x+y)$ where $A$ and $B$ are matricies and $x$ and $y$ are n-vectors\n",
    "\n",
    "a) compute $z$ as presented above. We see that matrix addtion $A+B$ and $x+y$ costs $mn$ and $n$ flops respecitively. Then we need to multiply their results which is between a matrix and a n-vector which cost 2nm flops. So we have $3nm +n$ flops but we drop the $n$ as $3nm$ is the leading term. So this operation costs $3nm$ flops. \n",
    "\n",
    "b) Cost if z's contents were distributed to be $z = Ax + Ay +Bx + By$. Here we see there are four matrix-vector multiplications which costs $2nm$ each for a total cost of $8nm$.\n",
    "\n",
    "c) We see that the by distributing we made the computatinal complexity greater aka worse. Therefore, we would want to use the first method as it requires fewer flops to perform the an equivalent computation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675b486-2e25-4a2e-8f32-f311b784452a",
   "metadata": {},
   "source": [
    "# 6.6.13\n",
    "\n",
    "Given a polynomial of degree n-1, $p(t) = c_1 +c_2t +...+ c_nt^{n-1}$, and its derivative of degree n-2,  $p'(t) = d_1 + d_2t + ... + d_{n-1}t^{n-2}$, find a matrix $D$ for which the matrix times the polynmoial results in the derivative of the polynomial, i.e., $d = Dc$. We can achieve this goal by letting D be a matrix with the first column being all zeros and of size $(n-1*1)$ and the rest of the columns of D can be represented by $diag(1,2,...,n-1)$. By doing this we get $d = Dc$ = ...\n",
    "\n",
    "$d_0 = c_1 * 0 = 0$ \n",
    "\n",
    "$d_1 = c_2 * 1 = c_2$\n",
    "\n",
    "$d_2 = c_3 * 2 = 3c_3$\n",
    "\n",
    "$d_3 = c_4 * 2 = 3c_4$\n",
    "\n",
    "...\n",
    "\n",
    "$d_n-1 = c_n * (n-1) = (n-1)c_n$\n",
    "\n",
    "which achieve the goal at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c747f7-19e3-4299-bf34-d8bd9925dc78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
