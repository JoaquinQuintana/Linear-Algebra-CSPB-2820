{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3698f6-6532-42bc-96d8-28bd2f22bef5",
   "metadata": {},
   "source": [
    "**5.1 a**\n",
    "To show the stacked vector $c_1 ... c_k$ is linearly independent we must show that $\\beta_1c_1+...+ \\beta_kc_k = 0$ which implies all the coefficents are zero or $\\beta_1 ... \\beta_k = 0$. \n",
    "\n",
    "We are told vectors $a_1 ... a_k$ are linearly independent and therefore $\\beta_1a_1+...+ \\beta_ka_k = 0$. This is, since $a_1 ... a_k$ are non-zero there is no possible way of obtaining the zero vector unless $\\beta_1a_1+...+ \\beta_ka_k = 0$. So returing to the stacked vector c we can show the following. \n",
    "\n",
    "$\\beta_1c_1+...+ \\beta_kc_k = 0$\n",
    "\n",
    "$ \\beta_1\\begin{bmatrix} a_1  \\\\ b_1 \\end{bmatrix}+ ... +$ $\\beta_k\\begin{bmatrix} a_k  \\\\ b_k \\end{bmatrix} = 0$\n",
    "\n",
    "$ \\begin{bmatrix} \\beta_1 a_1  \\\\ \\beta_1 b_1 \\end{bmatrix}, ... ,$ $\\begin{bmatrix} \\beta_k a_k  \\\\\\beta_k b_k \\end{bmatrix} = 0$ \n",
    "\n",
    "$\\beta_1a_1+...+ \\beta_ka_k = 0$\n",
    "\n",
    "By distributing $\\beta$ we see that $\\beta_1b_1+...+ \\beta_kb_k = 0$ and is therefore lineraly independent. Since vectors $a_1 ... a_k$ and $b_1 ... b_k$ are lineraly independent we can say $c_1 ... c_k$ is lineraly independent given $\\beta_1 ... \\beta_k = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e0e2f-cf15-44ba-803d-37e5b6e85383",
   "metadata": {},
   "source": [
    "**5.1 b**\n",
    "\n",
    "To show the stacked vector $c_1 ... c_k$ is linearly dependent we must show that $\\beta_1c_1+...+ \\beta_kc_k = 0$ with at least one $\\beta_i \\neq 0.$\n",
    "\n",
    "The answer is no. Unlike part a we do not have the case where $\\beta_i$ distributes out to with all $\\beta = 0$ \n",
    "\n",
    "Since the vectors $b_1 ... b_k$ can be linearly dependent or independet we cannot conclude the stacked vectors $c_1 ... c_k$ are linearly dependent.   \n",
    "\n",
    "For example let $b_1 ... b_k$ be equal to standard unit vectors $e_1 ... e_k$. Since $e_1 ... e_k$ are orthonormal we can say $b_1 ... b_k$ are linearly independent and since $c_i = \\begin{bmatrix} a_i  \\\\ b_i \\end{bmatrix}$ is a stacked vector we can conclude that $c_1 ... c_k$ is linearly independent regardless of $a_1 ... a_k$ being dependent. \n",
    "\n",
    "The other option is that $b_1 ... b_k$ is linearly dependent. For exmaple, lets assume $b_1 =  ... b_k = 0$ which by defintion is lineraly dependent. Since the collection of vectors $a_1 ... a_k$ and $b_1 ... b_k$ are linerly dependent we can say the stacked vectors $c_1 ... c_k$ are linearly dependent.\n",
    "\n",
    "%%%%%%%55\n",
    "\n",
    "The idea here is that you can have the set of vectors $b_1 ... b_k$ be either lineraly independent or dependent and therefore you cannot conclude $c_1 ... c_k$ is linearly dependent. This is, if $b_1 ... b_k$ are linearly independent vectors then $c_1 ... c_k$ is linearly independent and if $b_1 ... b_k$ are linearly dependent then $c_1 ... c_k$ is linearly dependent. Here is an example: \n",
    "\n",
    "Let $b_1 ... b_k$ be equal to standard unit vectors $e_1 ... e_k$. Since $e_1 ... e_k$ are orthonormal we can say $b_1 ... b_k$ are linearly independent and since $c_i = \\begin{bmatrix} a_i  \\\\ b_i \\end{bmatrix}$ is a stacked vector we can conclude that $c_1 ... c_k$ is linearly independent regardless of $a_1 ... a_k$ being dependent. \n",
    "\n",
    "The other option is that $b_1 ... b_k$ is linearly dependent. For example, lets assume $b_1 =  ... b_k = 0$ which by defintion is lineraly dependent. Since the collection of vectors $a_1 ... a_k$ and $b_1 ... b_k$ are linerly dependent we can say the stacked vectors $c_1 ... c_k$ are linearly dependent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64b680-f9aa-4bf1-bcbd-317c2091f628",
   "metadata": {},
   "source": [
    "# 5.2 \n",
    "A surprising discovery. An intern at a quantitative hedge fund examines the daily returns\n",
    "of around 400 stocks over one year (which has 250 trading days). She tells her supervisor\n",
    "that she has discovered that the returns of one of the stocks, Google (GOOG), can be\n",
    "expressed as a linear combination of the others, which include many stocks that are\n",
    "unrelated to Google (say, in a different type of business or sector).\n",
    "Her supervisor then says: “It is overwhelmingly unlikely that a linear combination of the\n",
    "returns of unrelated companies can reproduce the daily return of GOOG. So you’ve made\n",
    "a mistake in your calculations.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b16ba1-2e24-452d-9ac3-eaff025c81bd",
   "metadata": {},
   "source": [
    "First thing to note in this problem is the number of stocks = 400 and the number of days = 250. This can be translated to we have 250-vectors(n-vectors) each with $a_1, ..., a_k$ (k = number of stocks). From this information we can determine if this set of vectors is linearly independent or dependent using the Independence-dimension inequality. \n",
    "\n",
    "The Independence-dimension inequality states that a set of n-vectors $a_1,...a_k$ is linearly independent if $k \\leq n$ and if not it is linearly dependent. Here we see that n = 250 and k=400 or $n > k$ so our set of vectors is linearly dependent.  \n",
    "\n",
    "Next, a collection linearly independent vectors can be represented as $\\beta_1c_1+...+ \\beta_kc_k = 0$ where $\\beta_i$ is a scalar and not all $\\beta_i = 0$ in the linear combiantion. Due to this it is possible to create a set of vectors which could model the returns of a particular stock. \n",
    "\n",
    "This is, since google is a large company which is influenced by the world economy we would expect to be able to find a combination of stocks, on any given day, that could model the returns of Google's stock. Now just because on any given day we can create this model it doesn't mean that the same set of stocks on each day would be able to accurately compute the returns for the stock on any other day. \n",
    "\n",
    "So the professor is wrong as the student has a linearly dependent set of vectors which for that day can model the returns for Google's stock. However, the next day the student would most likely need to find a new linear combination of stocks to accurately represent Google's stock. So even though she is correct her solution is only valid for that day and fails the test of time. So in reality she should find a set of vectors which produces the same result but accurately from day to day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0237c18-5c07-4b87-8e31-c7ce4ecef93d",
   "metadata": {},
   "source": [
    "# 5.4\n",
    "This problem provides a linear combination of orthonormal vectors, x and we are asked to express the norm of the linear combination x in terms of its coefficients $\\beta_i$. \n",
    "\n",
    "We know that by squaring the norm of a vector we can represent the norm as the inner product of x with it self. So we get...\n",
    "\n",
    "$||x|| = x^Tx$\n",
    "\n",
    "Here I struggled to see how we could properly expand $x^Tx$ so that the inner product of pairs of vectors could be used. I ended up working the math out by hand using k = 2. By doing this I could see how all the terms, except the coefficients squared  = $\\beta_i^2$, end up being zero. Here is that example showing this...\n",
    "\n",
    "So from this simplified example we see that many of the terms are zero and we get $x^Tx = \\beta_1^2 + ... + \\beta_n^2 = \\sum_i^n \\beta_i^2$. The norm is given as...\n",
    "\n",
    "$||x|| = \\sqrt{x_i^2 + ... + x_n^2}$\n",
    "\n",
    "So by squaring by both sides and letting $x = \\beta$\n",
    "\n",
    "$ \\beta_i^2 + ... + \\beta_n^2 = ||\\beta||$\n",
    "\n",
    "The biggest takeaway from this problem was recognizing that the inner product of any two orthonormal vectors $a_i$ and $a_j$ when $j \\neq i$ are equal to zero, which makes sense as they are perpendicular. This is a very useful trick for simplifying this problem to show that the norm of a linear combination of orthonmormal vectors is simply $||x|| = ||\\beta||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082793f8-8f29-49f9-aba0-2414c8bc1b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eff3886-277f-45ca-9378-3ff95fe3d622",
   "metadata": {},
   "source": [
    "![expample](IMG-0068.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6250d7-135d-48e8-9b87-c3ab3b6ee9ca",
   "metadata": {},
   "source": [
    "# 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1265cc4-0a2c-46d5-8d57-5ce1569276c1",
   "metadata": {},
   "source": [
    "On page 93 we see how to express the cash flow c in expansion for using the basis $e_1,l_1,...,l_n$. This is,\n",
    "\n",
    "$c  = \\alpha_1e_1 +\\alpha_2l_1 + ... + \\alpha_nl_{n-1}$\n",
    "\n",
    "$ = \\begin{bmatrix} \\alpha_1 + \\alpha_2  \\\\ \\alpha_3 - (1+r)\\alpha_2 \\\\ ... \\\\  \\alpha_n - (1+r)\\alpha_{n-1}  \\\\ - (1+r)\\alpha_n \\end{bmatrix}$ \n",
    "\n",
    "From here I was eventually able to see that $c_i$ relates to the $i$th function in the expanded form and we can use this to find each $\\alpha_i$. For exmaple, $c_n = -(1+r)\\alpha_n$ and with rearragement we get $\\alpha_n = -\\frac{c_n}{(1+r)}$. \n",
    "\n",
    "From here we see that it is necessary to start with $\\alpha_n$ and work backwards finding $\\alpha_n-1$ then $\\alpha_n-2$. This is, we need the coefficent $\\alpha_i$ to compute $\\alpha_{i-1}$. Pretty quickly we see a pattern emerge which allows us to compute any $\\alpha_i$ for any $c_i$ ...\n",
    "\n",
    "$\\alpha_i = \\frac{c_i}{(1+r)^{n-1}}$\n",
    "\n",
    "Finally, we see when we reach $\\alpha_1$ we have computed all other coefficents and have $c_1 - \\alpha_2$ where $\\alpha_2$ is the difference of all the steps prior to computing $\\alpha_1$. So by subtracting $\\alpha_2$ from $c_1$ we are making all the negative signs positive and $\\alpha_1$ becomes the $c_1$ + $\\sum_{i = 2}^{n} \\frac{c_i}{1+r^{(i-1)}}$. This last computation is known as the net present value (NPV) of the cash flow as it shows that any cash flow can be replicated as an income in period 1 equal to NPV ($c_1$) plus the linear combintation of one period loans ($\\sum_{i = 2}^{n} \\frac{c_i}{1+r^{(i-1)}}$). Below I took the NPV cash flow example from the python companion notebook and show the $NPV  = c_1$ + $\\sum_{i = 2}^{n} \\frac{c_i}{1+r^{(i-1)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2948374-0067-4ff2-b532-0a3b7e82c444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPV computed using the dot product: 1.236162401468524\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$𝛼_0 = 0.1$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$𝛼_1 = 0.09524$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$𝛼_2 = 0.0907$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$𝛼_3 = 0.95022$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Latex\n",
    "import numpy as np\n",
    "c = np.array([0.1,0.1,0.1,1.1]) #cash flow vector\n",
    "n = len(c)\n",
    "r = 0.05 #5% per-period interest rate\n",
    "d = np.array([(1+r)**-i for i in range(n)])\n",
    "NPV = c @ d\n",
    "print(\"NPV computed using the dot product:\", NPV)\n",
    "\n",
    "#compute all alphas for the probelm and print them out\n",
    "alpha = [ c[i]*d[i] for i in range(len(c))]\n",
    "\n",
    "for i in range(len(c)):\n",
    "    display(Latex(f'$𝛼_{i} = {round(alpha[i],5)}$') )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca3783-5454-4f2e-84df-67b490de8bc6",
   "metadata": {},
   "source": [
    "Show that NPV is  $\\alpha_1 = c_1 - \\alpha_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad9a9e01-1a8d-4793-a924-19196271b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPV =  1.236162401468524\n"
     ]
    }
   ],
   "source": [
    "print(\"NPV = \", c[0] + sum(alpha[1:4])) #c[0] is c_1 and -alpha2 is the summation of all other alpha values from 1:end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ef95a-807d-416a-b218-97bcec386a28",
   "metadata": {},
   "source": [
    "# 5.5\n",
    "Any time we see orthogonal or perpendicular vectors I remember the inner product of two such vectors will be zero. So lets start there.\n",
    "\n",
    "**Given:** $(a-\\gamma b) \\perp b$\n",
    "\n",
    "**Lets say b = 0** then $(a-0) \\perp 0$ and we can write this as the inner product $a^T0 =0$ (inner product of vector a with a zero vector). This show that if b = 0 it does not matter what $\\gamma$ is as it is multiplied by 0 and disappears from our function. So if b = 0 we have $a^T0 = 0$ no matter what $\\gamma$ is and therefore $\\gamma$ is not unique in this case. \n",
    "\n",
    "**The other case is when $b \\neq 0 $**\n",
    "\n",
    "Here we can assume there is some component of the two vectors which are orthogonal given some $\\gamma$. We know the orthogonal portion of the two vectors inner product given some $\\gamma$ should equal 0. Again starting with $(a-\\gamma b)\\perp b$ we can solve for $\\gamma$ as follows...\n",
    "\n",
    "$(a-\\gamma b) \\perp b= (a-\\gamma b)^T b = 0$\n",
    "\n",
    "Starting with $= (a-\\gamma b)^T b $ we can distribute $^Tb$ as follows ...\n",
    "\n",
    "$= a^T b-\\gamma b^T b = 0 $\n",
    "\n",
    "**Note $b^T b = ||b||^2$**\n",
    "\n",
    "$ = a^T b-\\gamma ||b||^2 = 0$\n",
    "\n",
    "**Solve for $\\gamma$**\n",
    "\n",
    "$\\gamma =  \\frac{a^T b}{||b||^2}$\n",
    "\n",
    "Take aways: This problem points out how $\\gamma$ can be unique if b = 0 and is not if $b \\neq 0$. For the former we show that $\\gamma$ is simply multiplied by zero and therefore can be any number. This works because the inner product of a nonzero vector and a zero vector is always orthogonal. For the latter it is clear that their is only one $\\gamma$ which would make $  (a-\\gamma b)^T b  = 0$ and therefore is unique. In this second case $\\gamma =  \\frac{a^T b}{||b||^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b351a8d1-a8f2-4771-b671-e04a506bceb2",
   "metadata": {},
   "source": [
    "# 5.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a21546-6cb2-4b16-8fda-7c800e341e4c",
   "metadata": {},
   "source": [
    "For this problem I decided to code it up and see what the results were. Here we are using 100-vectors (n-vectors) each with $a_1, ..., a_k$ and ai has its first i entries equal to one, and the remaining entries zero. The code is followed by a brief summary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdbb81f-1961-4234-b8e3-610ea675ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function provided in python companion notebook\n",
    "import numpy as np\n",
    "def gram_schmidt(a):\n",
    "    q = []\n",
    "    for i in range(len(a)):\n",
    "    #orthogonalization\n",
    "        q_tilde = a[i]\n",
    "        for j in range(len(q)):\n",
    "            q_tilde = q_tilde - (q[j] @ a[i])*q[j]\n",
    "        #Test for dependennce\n",
    "        if np.sqrt(sum(q_tilde**2)) <= 1e-10:\n",
    "            print('Vectors are linearly dependent.')\n",
    "            print('GS algorithm terminates at iteration ', i+1)\n",
    "            return q\n",
    "            #Normalization\n",
    "        else:\n",
    "            q_tilde = q_tilde / np.sqrt(sum(q_tilde**2))\n",
    "            q.append(q_tilde)\n",
    "    print('Vectors are linearly independent.')\n",
    "    return q\n",
    "#define function for computing the norm of a vector\n",
    "def norm(nVector):\n",
    "    return sum(nVector**2)**0.5    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616c458b-b3cb-427a-ad88-92dd91ac9223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create vector ai has its first i entries equal to one, and the remaining entries zero. n = k = 100\n",
    "n = 100\n",
    "a = []\n",
    "for i in range(1,n+1):\n",
    "    a.append(np.concatenate((np.ones(i), np.zeros(n-i)), axis=None))\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb12bfc5-b70c-48ce-84ac-29179eb62cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are linearly independent.\n"
     ]
    }
   ],
   "source": [
    "#run gram-schmidt and lets see what we get \n",
    "q = gram_schmidt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa38e8e-617a-48d6-b58a-cca2762cd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    " import itertools\n",
    "#compute norm of q[i] and place in a list \n",
    "normQi = [norm(x) for x in q] \n",
    "#compute the inner product for all combinations of vectors in q\n",
    "l = range(0,n)# create a list of integers of size n. Use this with itertools to generate combinations of all indexes to iterate over\n",
    "innerProducts = [q[i[0]] @ q[i[1]] for i in itertools.combinations(l,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c62f68-5664-4509-addd-ecec9ca09a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the norm of each q[i] = 1 :  True\n",
      "Does the inner product of all possible pairs of q equal 0:  True\n"
     ]
    }
   ],
   "source": [
    "#confirm all norms equal 1\n",
    "norm_result = all(i == 1 for i in normQi)\n",
    "print(\"Does the norm of each q[i] = 1 : \",norm_result)\n",
    "#confirm all possible combinations of the innerproduct equal 0\n",
    "innerProductResults = all(i == 0 for i in innerProducts)\n",
    "print(\"Does the inner product of all possible pairs of q equal 0: \",innerProductResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415f969e-57ac-44e4-b518-6c41e660e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate unit vectors e_1, ..., e_n\n",
    "ei = [np.zeros(n) for i in range(100)]\n",
    "for i in range(100):\n",
    "    ei[i][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68399b34-6386-40ae-b10c-fe727cdbf939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "eiEqualqi = []\n",
    "for i in range(len(ei)):\n",
    "    if ei[i][i] != q[i][i]:\n",
    "        print(\"False e_i != q_1\")\n",
    "#print true if all vectors are equivalent \n",
    "print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e64f4-c134-4003-8eff-521f17990f3f",
   "metadata": {},
   "source": [
    "Here we show that Gram-Schmidt did not terminate early so we can say this set of vectors is linearly independent. If Gram-Schmidt had terminated early, this is some $q_j = 0$, then we can say the set of vector $a_1, ... , a_{j-1}$ are linerly independent and can be expressed in terms of $a_j$. At the end of the code here we confirm that the norm for all $q_i = 1$ and the inner product between any to $q_i$ and $q_j$ is zero as they all should be $\\perp$ when $i\\neq j$. In the last cell we confirm that $q_i == ei$ and since this set lineraly independent we know that all $\\beta_i = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a20c13-acb4-452c-9a46-8ba4a2354426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are linearly independent.\n",
      "Vectors are linearly dependent.\n",
      "GS algorithm terminates at iteration  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.70710678, 0.70710678]), array([-0.70710678,  0.70710678])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quiz Question\n",
    "#create set of vectors \n",
    "a1= np.array([1,1]); a2 = np.array([-3,2]) ;a3 = np.array([2,4])\n",
    "#create a set of vectors which is independent \n",
    "independent = [a1,a2];\n",
    "#take the set independent and concatenate #array a3 to it and call it dependent. \n",
    "#This is a stacked vector which is partially independent and dependent. See problem 1 for clarification\n",
    "dependents = [a1,a2,a3]\n",
    "#show that this set of vectors is lineraly indepedent\n",
    "gram_schmidt(independent)\n",
    "#show that this set of vectors is lineraly depedent\n",
    "gram_schmidt(dependents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e847ba62-deff-4d83-a5a0-0663a2e73291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are linearly independent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.70710678, 0.70710678]), array([-0.70710678,  0.70710678])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show that this set of vectors is lineraly indepedent\n",
    "gram_schmidt(independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6186edba-1ce6-4f8a-8998-0d8fe4c7851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are linearly dependent.\n",
      "GS algorithm terminates at iteration  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.70710678, 0.70710678]), array([-0.70710678,  0.70710678])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show that this set of vectors is lineraly depedent\n",
    "gram_schmidt(dependents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
